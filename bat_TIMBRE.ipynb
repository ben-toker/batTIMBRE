{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdf5storage\n",
    "from helpers import *\n",
    "from get_data import *\n",
    "from synchrony import *\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in the bat data (LFP and positional data)\n",
    "#### We first need to load in the LFP data, which in this case is stored in a MATLAB file. We can do this using ```hdf5storage```. The bat's positional data is stored in a matlab file (not accessible for public use), but luckily the accessors for this data can be found in ```dataset.py``` thanks to the Yartsev Lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data'\n",
    "bat_id = '32622'\n",
    "date = '231007'\n",
    "lfp_file_path = './data/ephys/32622_231007_lfp.mat'\n",
    "\n",
    "\n",
    "#Clean up position data (remove NaNs, etc.) and load in LFP from given file path\n",
    "lfp_mat, cleaned_pos, session = load_and_clean_bat_data(data_path, bat_id, date, lfp_file_path,use_cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time synchronization\n",
    "#### Before we get to the main attraction (the LFP data), we need to ensure our data is synchronized. To do this, we need to extract global timestamps from both the LFP and positional data and make sure they start at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfp_timestamps_edges, binned_pos, pos_timestamps, lfp_indices, valid_indices = sync_and_bin_data(lfp_mat, session,cleaned_pos)\n",
    "\n",
    "#lfp_timestamp_edges stores edges between timebins. this will be useful for aligning the LFP data with the position data\n",
    "#binned_pos is the cleaned position averaged over the timebins\n",
    "#valid_indices is a boolean array that marks the non-negative position timestamps\n",
    "#pos_timestamps is the cleaned and filtered timestamps of the position data\n",
    "#lfp_indices is a boolean array that marks the non-negative, decimated LFP timestamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of NaN values in binned_pos:\", np.isnan(binned_pos).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming binned_position is a 2D array with shape (N, 3) for [X, Y, Z]\n",
    "x_binned = binned_pos[:, 0]\n",
    "y_binned = binned_pos[:, 1]\n",
    "\n",
    "# If binned_position has a Z dimension, use a 3D plot\n",
    "if binned_pos.shape[1] == 3:\n",
    "    z_binned = binned_pos[:, 2]\n",
    "    \n",
    "    # 3D scatter plot\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(x_binned, y_binned, z_binned, c='blue', alpha=0.5, s=5)\n",
    "    ax.set_xlabel('X Position')\n",
    "    ax.set_ylabel('Y Position')\n",
    "    ax.set_zlabel('Z Position')\n",
    "    ax.set_title('Binned Position Data in 3D')\n",
    "else:\n",
    "    # 2D scatter plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(x_binned, y_binned, c='blue', alpha=0.5, s=5)\n",
    "    plt.xlabel('X Position')\n",
    "    plt.ylabel('Y Position')\n",
    "    plt.title('Binned Position Data in 2D')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inside of ```lfp_timestamps_edges```, we store the *edges* between timebins. We will use this to later to bin the position data; instead of downsampling the data like we did the LFP, we will average across bins (between two edges) of the LFP timebins to get synchronized data streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First few elements of binned_pos:\\n\", binned_pos[:, :5]) # NaN values at beginning and end are expected; position is not recorded when bat is not visible.\n",
    "\n",
    "print(\"First few LFP bins:\", lfp_timestamps_edges[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notice above that the LFP timestamp edges have N+1 the shape of the binned position. This makes sense and is expected; `lfp_timestamps_edges` contains the bins (which are stored in groups of two, i.e. the first bin is [0, 4514.4426] and so on) for which the position was binned into."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organizing behavioral data\n",
    "#### To better organize the binned flight data, we need to construct a flightID array which will contain all the binned positions for each flight, accounting for which feeder (or the perch) was visited for each data point entered in that flight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_data import get_flightID\n",
    "\n",
    "flightID = get_flightID(session, binned_pos, valid_indices, lfp_timestamps_edges, pos_timestamps,off_samples=200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract start and end positions\n",
    "start_positions = []\n",
    "end_positions = []\n",
    "\n",
    "for flight_number in np.unique(flightID[:, 0]):\n",
    "    # Get all data for the current flight\n",
    "    flight_data = flightID[flightID[:, 0] == flight_number]\n",
    "    \n",
    "    # Append start and end positions\n",
    "    start_positions.append(flight_data[0, 2:5])  # Columns 2, 3, 4 are X, Y, Z positions\n",
    "    end_positions.append(flight_data[-1, 2:5])   # Last row for end position\n",
    "\n",
    "# Convert lists to NumPy arrays for clustering\n",
    "start_positions = np.array(start_positions)\n",
    "end_positions = np.array(end_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Assuming start_positions and end_positions are already created\n",
    "# Remove rows with NaN values\n",
    "start_positions = start_positions[~np.isnan(start_positions).any(axis=1)]\n",
    "end_positions = end_positions[~np.isnan(end_positions).any(axis=1)]\n",
    "\n",
    "# Number of clusters (adjust based on observation)\n",
    "num_clusters = 3\n",
    "\n",
    "# Clustering start positions\n",
    "start_kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(start_positions)\n",
    "start_labels = start_kmeans.labels_\n",
    "\n",
    "# Clustering end positions\n",
    "end_kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(end_positions)\n",
    "end_labels = end_kmeans.labels_\n",
    "\n",
    "# Plot clusters for start positions\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.scatter(start_positions[:, 0], start_positions[:, 1], start_positions[:, 2], c=start_labels, cmap='viridis', s=50)\n",
    "ax.set_title('Clusters of Start Positions')\n",
    "ax.set_xlabel('X Position')\n",
    "ax.set_ylabel('Y Position')\n",
    "ax.set_zlabel('Z Position')\n",
    "\n",
    "# Plot clusters for end positions\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "ax.scatter(end_positions[:, 0], end_positions[:, 1], end_positions[:, 2], c=end_labels, cmap='viridis', s=50)\n",
    "ax.set_title('Clusters of End Positions')\n",
    "ax.set_xlabel('X Position')\n",
    "ax.set_ylabel('Y Position')\n",
    "ax.set_zlabel('Z Position')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming flightID[:, 1] contains the flight type codes\n",
    "flight_types = flightID[:, 1]\n",
    "\n",
    "# Define a mapping from flight type codes to their descriptions\n",
    "flight_type_mapping = {\n",
    "    0: \"Other/Unknown\",\n",
    "    1: \"Perch to Feeder 1\",\n",
    "    2: \"Feeder 1 to Perch\",\n",
    "    3: \"Perch to Feeder 2\",\n",
    "    4: \"Feeder 2 to Perch\",\n",
    "    5: \"Feeder 1 to Feeder 2\",\n",
    "    6: \"Feeder 2 to Feeder 1\"\n",
    "}\n",
    "\n",
    "# Count the occurrences of each unique flight type\n",
    "unique_types, counts = np.unique(flight_types, return_counts=True)\n",
    "\n",
    "# Calculate the percentage of each flight type\n",
    "total_samples = len(flight_types)\n",
    "percentages = (counts / total_samples) * 100\n",
    "\n",
    "# Print the percentage of each flight type using their descriptions\n",
    "print(\"Percentage of each flight type:\")\n",
    "for flight_type, percentage in zip(unique_types, percentages):\n",
    "    description = flight_type_mapping.get(int(flight_type), \"Unknown\")\n",
    "    print(f\"{description}: {percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract positions and classifications\n",
    "positions = flightID[:, 2:4]  # Assuming columns 2 and 3 contain the x and y positions\n",
    "classifications = [determine_feeder(pos) for pos in positions]\n",
    "\n",
    "# Convert classifications to numerical values for plotting\n",
    "classification_mapping = {'perch': 0, 'feeder1': 1, 'feeder2': 2, 'unknown': 3}\n",
    "numerical_classifications = [classification_mapping[cls] for cls in classifications]\n",
    "\n",
    "# Plot the positions with different colors for each classification\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(positions[:, 0], positions[:, 1], c=numerical_classifications, cmap='viridis', alpha=0.6)\n",
    "plt.colorbar(ticks=[0, 1, 2, 3], label='Classification')\n",
    "plt.clim(-0.5, 3.5)\n",
    "plt.xlabel('X Position')\n",
    "plt.ylabel('Y Position')\n",
    "plt.title('Position Classification Visualization')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LFP extraction and downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfp_bat_combined = extract_and_downsample_lfp_data(lfp_mat) #uses scipy decimate to downsample LFP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imported raw LFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synchrony import plot_raw_lfp\n",
    "\n",
    "# Example usage\n",
    "plot_raw_lfp(lfp_bat_combined, n_channels=192, start_time=0, end_time=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import welch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the first channel from lfp_bat_combined\n",
    "first_channel = lfp_bat_combined[:, 0]\n",
    "\n",
    "# Compute the power spectral density using Welch's method\n",
    "frequencies, psd = welch(first_channel, fs=25, nperseg=1024)\n",
    "\n",
    "# Plot the PSD\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogy(frequencies, psd)\n",
    "plt.title('Power Spectral Density of the First LFP Channel')\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Power/Frequency (dB/Hz)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import welch\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assume lfp_bat_combined is your LFP data matrix with shape (samples, channels)\n",
    "\n",
    "# Step 1: Set up the plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Step 2: Loop through each channel to compute and plot the PSD of the first principal component\n",
    "n_channels = lfp_bat_combined.shape[1]\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, n_channels))  # Use a more varied colormap like 'tab20'\n",
    "\n",
    "# Choose to only plot every nth channel for clarity\n",
    "nth_channel = max(1, n_channels // 60)  # Plot approximately 20 lines\n",
    "\n",
    "for channel in range(0, n_channels, nth_channel):\n",
    "    # Extract data for the current channel\n",
    "    channel_data = lfp_bat_combined[:, channel].reshape(-1, 1)\n",
    "\n",
    "    # Apply PCA (though it's one-dimensional, we still follow this step for uniformity)\n",
    "    pca = PCA()\n",
    "    principal_component = pca.fit_transform(channel_data)\n",
    "\n",
    "    # Extract the first principal component (since PCA on a single channel returns the same data)\n",
    "    first_principal_component = principal_component[:, 0]\n",
    "\n",
    "    # Compute the PSD using Welch's method\n",
    "    frequencies, psd = welch(first_principal_component, fs=25, nperseg=1024)\n",
    "\n",
    "    # Plot each PSD with a unique color and line style\n",
    "    plt.semilogy(frequencies, psd, color=colors[channel % len(colors)], alpha=0.8)\n",
    "\n",
    "\n",
    "# Step 3: Customize the plot\n",
    "# Adjust the x and y position to place the text in a clearer area\n",
    "plt.text(6, 1e7, f\"Note: Every {nth_channel}th channel is visualized\", fontsize=10, ha='center', bbox=dict(facecolor='white', alpha=0.6))\n",
    "plt.title('Power Spectral Density of the First Principal Component for Selected Channels')\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Power/Frequency (dB/Hz)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once LFP is loaded in and downsampled, we can apply a filter and Hilbert transform to get our complex-valued LFP!\n",
    "\n",
    "#### *Note: At 25hz, a signal has at most 12.5hz frequency of usable data. Given this property, we don't need to do a bandpass filter (to cap out the high and low range). As such, we only need to do a highpass filter at 1hz.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFPs = filter_data(lfp_bat_combined, 1, fs=25, filt_type='high', use_hilbert=True) \n",
    "LFPs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFPs = LFPs[lfp_indices]\n",
    "LFPs.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We now have our processed LFP. `LFPs` contains the filtered and (Hilbert) transformed LFP data for all of the valid `binned_pos` entries. However, we are mainly interested in the bat flights, which are just a *fraction* of the total of `binned_pos`. To filter out the non-flight entries from the LFP, we will apply a similar filtering method as we did in `get_flightID` with a `get_flightLFP` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_data import get_flightLFP\n",
    "\n",
    "flightLFP = get_flightLFP(session, LFPs, valid_indices, lfp_timestamps_edges, pos_timestamps, off_samples=125) # Make sure off_samples is the same for flightID and flightLFP.\n",
    "flightLFP.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-TIMBRE analysis\n",
    "##### We're going to apply logistic regression to give us a baseline to compare TIMBRE to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE  # For oversampling minority classes\n",
    "\n",
    "# Step 1: Extract the amplitude envelope (magnitude) from the Hilbert-transformed LFP data\n",
    "lfp_features = np.abs(flightLFP)  # Use the amplitude envelope\n",
    "\n",
    "# Step 2: Prepare the position labels\n",
    "position_labels = flightID[:, 1].astype(int)\n",
    "\n",
    "# Step 3: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(lfp_features, position_labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 4: Scale the data to improve model performance and convergence\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 5: Address class imbalance using SMOTE to oversample minority classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Step 6: Create and fit the logistic regression model with class weights\n",
    "model = LogisticRegression(max_iter=2000, solver='saga', class_weight='balanced')  # Increased iterations, used 'saga' solver, and balanced class weights\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 8: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Classification Report:\\n{report}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying TIMBRE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from TIMBRE import TIMBRE\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Define class names matching your labels (labels from 0 to 5)\n",
    "class_names = [\n",
    "    'Perch to Feeder 1',       # Label 0\n",
    "    'Feeder 1 to Perch',       # Label 1\n",
    "    'Perch to Feeder 2',       # Label 2\n",
    "    'Feeder 2 to Perch',       # Label 3\n",
    "    'Feeder 1 to Feeder 2',    # Label 4\n",
    "    'Feeder 2 to Feeder 1'     # Label 5\n",
    "]\n",
    "\n",
    "# Initialize parameters\n",
    "n_folds = 5\n",
    "hidden_sizes = [3, 6, 12, 24]\n",
    "all_accuracies = []\n",
    "all_cm = {}\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    print(f\"\\nEvaluating hidden size: {hidden_size} nodes\")\n",
    "    fold_accuracies = []\n",
    "    cm_total = None\n",
    "\n",
    "    for which_fold in range(n_folds):\n",
    "        print(f\"  Fold {which_fold + 1}/{n_folds}\")\n",
    "\n",
    "        # Get train and test indices\n",
    "        test_inds, train_inds = test_train_bat(flightID, n_folds, which_fold)\n",
    "\n",
    "        # Whiten LFPs\n",
    "        wLFPs, _, _ = whiten(LFPs, train_inds)\n",
    "\n",
    "        # Adjust labels to start from 0\n",
    "        labels = flightID[:, 1].astype(int) - 1\n",
    "\n",
    "        # Train the TIMBRE model\n",
    "        m, _, _ = TIMBRE(\n",
    "            wLFPs, labels, test_inds, train_inds,\n",
    "            hidden_nodes=hidden_size, is_categorical=True\n",
    "        )\n",
    "\n",
    "        # Get predictions on test data\n",
    "        output_layer = layer_output(wLFPs[test_inds], m, -1)\n",
    "        predictions = np.argmax(output_layer, axis=1)\n",
    "        true_labels = labels[test_inds]\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = np.mean(predictions == true_labels)\n",
    "        fold_accuracies.append(accuracy)\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        labels_list = np.arange(len(class_names))  # Labels from 0 to 5\n",
    "        cm = confusion_matrix(\n",
    "            true_labels, predictions, labels=labels_list\n",
    "        )\n",
    "\n",
    "        # Accumulate confusion matrices\n",
    "        if cm_total is None:\n",
    "            cm_total = cm\n",
    "        else:\n",
    "            cm_total += cm\n",
    "\n",
    "        # Classification report\n",
    "        report = classification_report(\n",
    "            true_labels, predictions,\n",
    "            labels=labels_list,\n",
    "            target_names=class_names, zero_division=0\n",
    "        )\n",
    "        print(f\"Classification Report for Fold {which_fold + 1}:\\n{report}\")\n",
    "\n",
    "    # Average accuracy\n",
    "    avg_accuracy = np.mean(fold_accuracies)\n",
    "    all_accuracies.append(avg_accuracy)\n",
    "    all_cm[hidden_size] = cm_total / n_folds\n",
    "\n",
    "    print(f\"Average accuracy for hidden size {hidden_size}: {avg_accuracy:.4f}\")\n",
    "\n",
    "# Plot average accuracy vs hidden layer size\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(hidden_sizes, all_accuracies, marker='o')\n",
    "plt.title('Model Accuracy vs Hidden Layer Size')\n",
    "plt.xlabel('Number of Hidden Nodes')\n",
    "plt.ylabel('Average Accuracy over Folds')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot average confusion matrices for each hidden size\n",
    "fig, axs = plt.subplots(2, 2, figsize=(20, 15))\n",
    "fig.suptitle('Average Confusion Matrices for Bat Flight End Position Prediction', fontsize=16)\n",
    "\n",
    "for idx, hidden_size in enumerate(hidden_sizes):\n",
    "    cm_avg = all_cm[hidden_size]\n",
    "\n",
    "    # Normalize confusion matrix to show percentages\n",
    "    cm_normalized = cm_avg.astype('float') / cm_avg.sum(axis=1)[:, np.newaxis]\n",
    "    cm_normalized = np.nan_to_num(cm_normalized)  # Replace NaNs with zeros if any class has zero samples\n",
    "\n",
    "    ax = axs[idx // 2, idx % 2]\n",
    "    sns.heatmap(\n",
    "        cm_normalized, annot=True, fmt='.2f', ax=ax,\n",
    "        xticklabels=class_names, yticklabels=class_names, cmap='Blues'\n",
    "    )\n",
    "    ax.set_title(f'Confusion Matrix (Hidden Nodes: {hidden_size})')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_pos_bat(positions, n_bins, train_inds):\n",
    "    min_pos = np.min(positions[train_inds], axis=0)\n",
    "    max_pos = np.max(positions[train_inds], axis=0)\n",
    "    return np.floor((positions - min_pos) / (max_pos - min_pos) * n_bins).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from TIMBRE import TIMBRE\n",
    "import numpy as np\n",
    "import helpers  # Assuming you have a helpers module\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "fig, axs = plt.subplots(4, 4, figsize=(20, 15))\n",
    "fig.suptitle('TIMBRE Model Performance for Bat Flight Position Prediction', fontsize=16)\n",
    "\n",
    "n_folds = 5\n",
    "which_fold = 0\n",
    "n_bins = 20  # Adjust as needed\n",
    "\n",
    "# Step 1: Obtain test and train indices\n",
    "test_inds, train_inds = test_train_bat(flightID, n_folds, which_fold)\n",
    "\n",
    "# Step 2: Whiten the LFPs\n",
    "wLFPs, _, _ = helpers.whiten(LFPs, train_inds)\n",
    "\n",
    "# Step 3: Extract positions and apply PCA\n",
    "positions = flightID[:, 2:5]  # X, Y, Z positions\n",
    "pca = PCA(n_components=1)\n",
    "positions_1d = pca.fit_transform(positions).flatten()\n",
    "\n",
    "# Step 4: Bin the 1D positions\n",
    "pos_bins = np.linspace(positions_1d.min(), positions_1d.max(), n_bins + 1)\n",
    "pos_binned = np.digitize(positions_1d, bins=pos_bins) - 1\n",
    "labels = pos_binned\n",
    "\n",
    "# Step 5: Training and Plotting\n",
    "titles = ['Projection (real part)', 'Amplitude', 'Softmax 1', 'Softmax 2 (Output)']\n",
    "for i in range(axs.shape[0]):\n",
    "    hidden_nodes = 3 * 2 ** i\n",
    "    print(f\"Training network {i + 1} of {axs.shape[0]} (hidden layer size {hidden_nodes})\")\n",
    "    \n",
    "    # Train the TIMBRE model\n",
    "    m, _, _ = TIMBRE(wLFPs, labels, test_inds, train_inds, hidden_nodes=hidden_nodes)\n",
    "    \n",
    "    for j in range(axs.shape[1]):\n",
    "        # Calculate layer's response to input, using only test data\n",
    "        p = helpers.layer_output(wLFPs[test_inds], m, j)\n",
    "        \n",
    "        if j == 0:\n",
    "            p = p[:, :p.shape[1] // 2]\n",
    "            axs[i, 0].set_ylabel(f'{hidden_nodes} features')\n",
    "        \n",
    "        if i == 0:\n",
    "            axs[0, j].set_title(titles[j])\n",
    "        \n",
    "        # Compute mean response per position bin\n",
    "        mean_response = helpers.accumarray(labels[test_inds], p)\n",
    "        \n",
    "        # Plot the mean response\n",
    "        axs[i, j].plot(mean_response)\n",
    "        axs[i, j].autoscale(enable=True, axis='both', tight=True)\n",
    "        \n",
    "        if i < axs.shape[0] - 1:\n",
    "            axs[i, j].set_xticks([])\n",
    "        else:\n",
    "            axs[i, j].set_xlabel('Position along Principal Component')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
