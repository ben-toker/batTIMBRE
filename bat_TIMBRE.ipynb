{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdf5storage\n",
    "import helpers\n",
    "import get_data\n",
    "import numpy as np\n",
    "from bat.helpers_bat import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the bat data (LFP and positional data)\n",
    "We first need to load in the LFP data, which in this case is stored in a MATLAB file. We can do this using ```hdf5storage```. The bat's positional data is stored in a different file, but luckily the accessors for this were provided by the Yartsev lab (thanks Kevin Qi!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading bat LFP data\n",
    "lfp_mat = hdf5storage.loadmat('./bat/ephys/32622_231007_lfp.mat')\n",
    "\n",
    "# Check the structure of lfp_mat\n",
    "print(\"Structure of lfp_mat:\", type(lfp_mat['lfp']), lfp_mat['lfp'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './bat' # Replace this\n",
    "bat_id = '32622'\n",
    "date =  '231007'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = FlightRoomSession(data_path, bat_id, date, use_cache = False) # use_cache = True to save time on future loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = session.cortex_data.bat_pos # (num_timepoints, XYZ)\n",
    "pos.shape\n",
    "pos[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_pos = np.copy(pos)\n",
    "# still position on walls are nans (dont interpolate large gaps of nans)\n",
    "cleaned_pos[:, 0] = interpolate_nans(pos[:, 0])\n",
    "cleaned_pos[:, 1] = interpolate_nans(pos[:, 1])\n",
    "cleaned_pos[:, 2] = interpolate_nans(pos[:, 2])\n",
    "cleaned_pos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time synchronization\n",
    "Before we get to the main attraction (the LFP data), we need to ensure our data is synchronized. To do this, we need to extract global timestamps from both the LFP and positional data and make sure they start at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = lfp_mat['global_sample_timestamps_usec'] #global timestamps in microseconds\n",
    "\n",
    "np.diff(timestamps.flatten()) #we will see a 400 microsecond difference between each timestamp,\n",
    "#which means that the sampling rate is 2500 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we filtered out the negative timestamps from the position recording relative to the global timestamp start. We simply had to filter out the corresponding samples in position to synchronize. Now we're ready to make timebins from the LFP timestamp data and bin the positional data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import decimate\n",
    "\n",
    "lfp_timestamps_decimated_bins = decimate(timestamps.flatten(), 100) #decimate from 2500 Hz to 25 Hz (100x decimation)\n",
    "lfp_timestamps_decimated_bins.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfp_indices = lfp_timestamps_decimated_bins > 0\n",
    "lfp_timestamps_decimated_bins = lfp_timestamps_decimated_bins[lfp_indices] # lop off negative timestamps\n",
    "lfp_timestamps_decimated_bins = np.insert(lfp_timestamps_decimated_bins, 0, 0) # insert 0 at the beginning\n",
    "\n",
    "lfp_timestamps_decimated_bins.shape\n",
    "#lfp_timestamps_decimated_bins[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_timestamps = session.cortex_data.cortex_global_sample_timestamps_sec * 1e6 #converting to microseconds (usec)\n",
    "\n",
    "valid_indices = pos_timestamps > 0\n",
    "\n",
    "pos_timestamps = pos_timestamps[valid_indices] # lop off negative timestamps \n",
    "\n",
    "np.diff(pos_timestamps) \n",
    "#pos_timestamps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out cluster 1 flight position and LFP timestamps\n",
    "def exclude_cluster1_indices(session, pos_timestamps, lfp_timestamps):\n",
    "    cluster1_pos_indices = []\n",
    "    cluster1_lfp_indices = []\n",
    "    \n",
    "    for flight_number in range(session.cortex_data.num_flights):\n",
    "        if session.cortex_data.cluster_ids[flight_number] == 1:\n",
    "            flight = session.flights[flight_number]\n",
    "            start_idx = flight.timebin_start_idx\n",
    "            end_idx = flight.timebin_end_idx\n",
    "            cluster1_pos_indices.extend(range(start_idx, end_idx + 1))\n",
    "\n",
    "    # Convert original indices to timestamps\n",
    "    cluster1_pos_timestamps = pos_timestamps[cluster1_pos_indices]\n",
    "\n",
    "    # Create masks to exclude these timestamps\n",
    "    pos_mask = np.isin(pos_timestamps, cluster1_pos_timestamps, invert=True)\n",
    "    lfp_mask = np.isin(lfp_timestamps, cluster1_pos_timestamps, invert=True)\n",
    "\n",
    "    # Apply masks to timestamps\n",
    "    filtered_pos_indices = np.where(pos_mask)[0]\n",
    "    filtered_lfp_indices = np.where(lfp_mask)[0]\n",
    "\n",
    "    return filtered_pos_indices, filtered_lfp_indices\n",
    "\n",
    "# Call the function with the session, pos_timestamps, and lfp_timestamps\n",
    "filtered_pos_indices, filtered_lfp_indices = exclude_cluster1_indices(session, pos_timestamps, lfp_timestamps_decimated_bins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##pos_timestamps = pos_timestamps[valid_indices]\n",
    "#pos_timestamps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_pos = cleaned_pos[valid_indices] # lop off the corresponding positions\n",
    "cleaned_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_pos[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin positional data using the provided label_timebins function\n",
    "binned_pos_x = label_timebins(lfp_timestamps_decimated_bins, cleaned_pos[:, 0], pos_timestamps, is_discrete=False)\n",
    "binned_pos_y = label_timebins(lfp_timestamps_decimated_bins, cleaned_pos[:, 1], pos_timestamps, is_discrete=False)\n",
    "binned_pos_z = label_timebins(lfp_timestamps_decimated_bins, cleaned_pos[:, 2], pos_timestamps, is_discrete=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the lengths match\n",
    "min_length = min(len(binned_pos_x), len(binned_pos_y), len(binned_pos_z), len(lfp_timestamps_decimated_bins))\n",
    "# Truncate arrays to the minimum length\n",
    "binned_pos_x = binned_pos_x[:min_length]\n",
    "binned_pos_y = binned_pos_y[:min_length]\n",
    "binned_pos_z = binned_pos_z[:min_length]\n",
    "lfp_timestamps_decimated_bins = lfp_timestamps_decimated_bins[:min_length]\n",
    "\n",
    "# Initialize binned_pos array with the truncated length\n",
    "binned_pos = np.zeros((3, min_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_pos = np.zeros((3,lfp_timestamps_decimated_bins.shape[0]))\n",
    "# Assign binned positions to binned_pos array\n",
    "binned_pos[0, :] = binned_pos_x\n",
    "binned_pos[1, :] = binned_pos_y\n",
    "binned_pos[2, :] = binned_pos_z\n",
    "\n",
    "# Verify the contents of binned_pos\n",
    "print(f\"binned_pos shape: {binned_pos.shape}\")\n",
    "print(f\"First few entries in binned_pos:\\n{binned_pos[:, :5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_behavior = session.cortex_data\n",
    "labels = np.full([flight_behavior.num_cortex_timebins], 0)\n",
    "cluster_flights = session.get_flights_by_cluster((2,9))\n",
    "cluster_flights_id = [flight.cluster_id for flight in cluster_flights]\n",
    "for i_flight in range(len(cluster_flights)):\n",
    "    s = cluster_flights[i_flight].timebin_start_idx\n",
    "    e = cluster_flights[i_flight].timebin_end_idx\n",
    "    labels[s:e] = i_flight + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are creating labels (timebin_labels) to associate which timebins are related to which flight so we can access them accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels[valid_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.interpolate as interpolate\n",
    "def find_multi_label_bins(spk_timebins, labels, label_timestamps_sec):\n",
    "    bin_indices = np.digitize(label_timestamps_sec, spk_timebins) - 1\n",
    "    valid_mask = (bin_indices >= 0) & (bin_indices < len(spk_timebins) - 1)\n",
    "    bin_indices = bin_indices[valid_mask]\n",
    "    valid_labels = labels[valid_mask]\n",
    "   \n",
    "    # Find unique bin-label combinations\n",
    "    unique_combinations, counts = np.unique(np.column_stack((bin_indices, valid_labels)), axis=0, return_counts=True)\n",
    "   \n",
    "    # Count the number of unique labels for each bin\n",
    "    bin_label_counts = np.zeros(len(spk_timebins) - 1, dtype=int)\n",
    "    np.add.at(bin_label_counts, unique_combinations[:, 0], 1)\n",
    "   \n",
    "    # Find bins with multiple different labels\n",
    "    bins_with_multi_unique_labels = np.where(bin_label_counts > 1)[0]\n",
    "    #print(unique_combinations, counts)\n",
    "\n",
    "    return bins_with_multi_unique_labels, unique_combinations, counts\n",
    "\n",
    "def label_timebins(spk_timebins, labels, label_timestamps_sec, is_discrete):\n",
    "    \"\"\"\n",
    "    result = label_timebins([0,2,4,6,8], np.array([2,2,4,5,6,6,6,6,1,2,2,4,3,3,3]), np.array([0.5,1,1.5,1.6,1.5,1,1,1,3.5,6.5,6.6,6.5,6.4,6.5,6.6]), is_discrete=True)\n",
    "    print(result)\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays\n",
    "    spk_timebins = np.array(spk_timebins)\n",
    "    labels = np.array(labels)\n",
    "    label_timestamps_sec = np.array(label_timestamps_sec)\n",
    "   \n",
    "    # Calculate the midpoints of spk_timebins\n",
    "    spk_midpoints = (spk_timebins[:-1] + spk_timebins[1:]) / 2\n",
    "   \n",
    "    if is_discrete:\n",
    "        # For discrete labels, use nearest neighbor interpolation\n",
    "        f = interpolate.interp1d(label_timestamps_sec, labels, kind='nearest',\n",
    "                                 bounds_error=False, fill_value=0)\n",
    "        resampled_labels = f(spk_midpoints)\n",
    "       \n",
    "        # Find bins with multiple different labels\n",
    "        multi_label_bins, unique_combinations, counts = find_multi_label_bins(spk_timebins, labels, label_timestamps_sec)\n",
    "        #print(multi_label_bins)\n",
    "       \n",
    "        # Correct labels for bins with multiple different labels\n",
    "        if len(multi_label_bins) > 0:\n",
    "            for bin_index in multi_label_bins:\n",
    "                bin_label_counts = np.argmax(counts[unique_combinations[:, 0] == bin_index])\n",
    "                #print(unique_combinations[unique_combinations[:, 0] == bin_index,:])\n",
    "                resampled_labels[bin_index] = unique_combinations[unique_combinations[:, 0] == bin_index, 1][bin_label_counts]\n",
    "       \n",
    "        # Set labels to 0 for bins without any labels\n",
    "        valid_bins = np.digitize(label_timestamps_sec, spk_timebins) - 1\n",
    "        valid_bins = valid_bins[(valid_bins >= 0) & (valid_bins < len(spk_timebins) - 1)]\n",
    "        invalid_bins = np.setdiff1d(np.arange(len(spk_timebins) - 1), valid_bins)\n",
    "        resampled_labels[invalid_bins] = 0\n",
    "       \n",
    "    else:\n",
    "        # For continuous labels, use linear interpolation\n",
    "        f = interpolate.interp1d(label_timestamps_sec, labels, kind='linear',\n",
    "                                 bounds_error=False, fill_value=np.nan)\n",
    "        resampled_labels = f(spk_midpoints)\n",
    "       \n",
    "        # Set labels to NaN for bins without any labels\n",
    "        valid_bins = np.digitize(label_timestamps_sec, spk_timebins) - 1\n",
    "        valid_bins = valid_bins[(valid_bins >= 0) & (valid_bins < len(spk_timebins) - 1)]\n",
    "        invalid_bins = np.setdiff1d(np.arange(len(spk_timebins) - 1), valid_bins)\n",
    "        resampled_labels[invalid_bins] = np.nan\n",
    "   \n",
    "    return resampled_labels\n",
    "\n",
    "\n",
    "def label_in_timebin(timebin_edges, label_timestamps):\n",
    "    timebin_edges = np.array(timebin_edges)\n",
    "    label_timestamps = np.array(label_timestamps)\n",
    "   \n",
    "    # Create an array of booleans, one for each timebin\n",
    "    result = np.zeros(len(timebin_edges) - 1, dtype=bool)\n",
    "   \n",
    "    # Use numpy's digitize to find which bin each label falls into\n",
    "    bin_indices = np.digitize(label_timestamps, timebin_edges) - 1\n",
    "   \n",
    "    # Filter out any indices that are out of bounds\n",
    "    valid_indices = (bin_indices >= 0) & (bin_indices < len(result))\n",
    "    bin_indices = bin_indices[valid_indices]\n",
    "   \n",
    "    # Set the corresponding bins to True\n",
    "    result[bin_indices] = True\n",
    "   \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "timebin_labels = label_timebins(lfp_timestamps_decimated_bins, labels, pos_timestamps, is_discrete=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(binned_pos_x[timebin_labels >0], binned_pos_y[timebin_labels >0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flightID(session, binned_pos, binned_indices):\n",
    "    flight_info = []\n",
    "    flight_number = 1\n",
    "\n",
    "    # Initialize a flight number array with default value -1\n",
    "    flight_numbers = -np.ones(binned_pos.shape[1], dtype=int)\n",
    "    feeders_visited = -np.ones(binned_pos.shape[1], dtype=int)\n",
    "\n",
    "    # Iterate through each flight\n",
    "    for flight_number in range(session.num_flights):\n",
    "        if session.cortex_data.cluster_ids[flight_number] == 1:\n",
    "            continue  # Exclude cluster 1\n",
    "\n",
    "        flight = session.flights[flight_number]\n",
    "        start_idx = flight.timebin_start_idx\n",
    "        end_idx = flight.timebin_end_idx\n",
    "\n",
    "        # Convert original indices to binned indices\n",
    "        binned_start_idx = binned_indices[start_idx]\n",
    "        binned_end_idx = binned_indices[end_idx]\n",
    "\n",
    "        # Ensure indices are within bounds\n",
    "        if binned_end_idx >= binned_pos.shape[1]:\n",
    "            print(f\"Skipping flight with end_idx {binned_end_idx} as it exceeds bounds.\")\n",
    "            continue\n",
    "\n",
    "        # Determine which feeder the bat visited based on the x and y coordinates of the last position\n",
    "        end_x = binned_pos[0, binned_end_idx]\n",
    "        end_y = binned_pos[1, binned_end_idx]\n",
    "\n",
    "        if end_y > 0 and end_x < 0:\n",
    "            feeder_visited = 0  # Perch\n",
    "        elif end_y > 0 and end_x > 0:\n",
    "            feeder_visited = 1  # Feeder 1\n",
    "        elif end_y < 0 and end_x > 0:\n",
    "            feeder_visited = 2  # Feeder 2\n",
    "        else:\n",
    "            feeder_visited = -1  # In case it doesn't match any criteria (should not happen)\n",
    "\n",
    "        # Assign the flight number and feeder visited to each sample in the binned_pos array\n",
    "        flight_numbers[binned_start_idx:binned_end_idx + 1] = flight_number + 1\n",
    "        feeders_visited[binned_start_idx:binned_end_idx + 1] = feeder_visited\n",
    "\n",
    "    # Create the flightID array with all samples included\n",
    "    for idx in range(binned_pos.shape[1]):\n",
    "        flight_info.append([flight_numbers[idx], feeders_visited[idx], binned_pos[0, idx], binned_pos[1, idx], binned_pos[2, idx]])\n",
    "\n",
    "    # Convert to numpy array\n",
    "    flightID = np.array(flight_info)\n",
    "\n",
    "    return flightID\n",
    "# Call get_flightID with the session and binned_pos\n",
    "flightID = get_flightID(session, binned_pos, binned_indices)\n",
    "\n",
    "# Verify the contents of flightID\n",
    "print(f\"flightID shape: {flightID.shape}\")\n",
    "print(f\"First few entries in flightID:\\n{flightID[:5]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LFP extraction and downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract subarrays and check their structure\n",
    "lfp_data_1 = lfp_mat['lfp'][0, 0]\n",
    "lfp_data_2 = lfp_mat['lfp'][0, 1]\n",
    "\n",
    "print(f\"Type of lfp_data_1: {type(lfp_data_1)}, Shape of lfp_data_1: {lfp_data_1.shape}\")\n",
    "print(f\"Type of lfp_data_2: {type(lfp_data_2)}, Shape of lfp_data_2: {lfp_data_2.shape}\")\n",
    "\n",
    "n_channels = lfp_data_1.shape[0] #same # of channels for lfp_data_1 and lfp_data_2 (change if not the case)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# bat LFP data sampled at 2500 Hz\n",
    "lfp_bat_1 = get_data.get_LFP_from_mat(lfp_data_1,n_channels,2500)\n",
    "lfp_bat_2 = get_data.get_LFP_from_mat(lfp_data_2,n_channels,2500)\n",
    "\n",
    "lfp_bat_combined = np.concatenate((lfp_bat_1, lfp_bat_2), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"lfp_bat_1 shape:\", lfp_bat_1.shape) # (n_samples, n_channels)\n",
    "print(\"lfp_bat_2 shape:\", lfp_bat_2.shape) # (n_samples, n_channels)\n",
    "print(\"lfp_bat_combined shape:\", lfp_bat_combined.shape) # (n_samples, 2*n_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once LFP is loaded in, we can downsample to 25hz and apply a Hilbert transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFPs = helpers.filter_data(lfp_bat_combined, 0.1, fs=25, use_hilbert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFPs.shape #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_LFP = LFPs[lfp_indices]\n",
    "bin_LFP=bin_LFP[:min_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_LFP.shape # (# of binned samples, n_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from TIMBRE import TIMBRE\n",
    "from bat.helpers_bat import *\n",
    "\n",
    "fig, axs = plt.subplots(4, 4, figsize=(20, 5))\n",
    "\n",
    "n_folds = 5\n",
    "which_fold = 0\n",
    "num_samples_at_end = 5  # Number of samples at the end of each flight to use for classification\n",
    "\n",
    "test_inds, train_inds = test_train_bat(flightID, n_folds, which_fold, num_samples_at_end)\n",
    "\n",
    "wLFPs, _, _ = helpers.whiten(bin_LFP, train_inds)\n",
    "\n",
    "# Verify the shapes of the indices arrays\n",
    "#print(f\"Test indices shape: {test_inds.shape}\")\n",
    "#print(f\"Train indices shape: {train_inds.shape}\")\n",
    "\n",
    "# Assuming `wLFPs` and `flightID` are used in TIMBRE\n",
    "#print(f\"wLFPs shape: {wLFPs.shape}\")\n",
    "#print(f\"flightID shape: {flightID.shape}\")\n",
    "\n",
    "\n",
    "#n_bins = 20\n",
    "#pos_binned = helpers.group_by_pos(data['lapID'][:, 4], n_bins, train_inds)  # Convert position along the track into discrete bins.\n",
    "#arm_and_pos_binned = data['lapID'][:, 1] * n_bins + pos_binned  # Represent arm x position as integer between 0-19 (arm 1), 20-39 (arm 2), 40-59 (arm 3)\n",
    "# Additional debug statements to verify input to TIMBRE\n",
    "print(f\"X (wLFPs) shape: {wLFPs.shape}\")\n",
    "print(f\"Y (flightID[:, 1]) shape: {flightID[:, 1].shape}\")\n",
    "print(f\"inds_train shape: {train_inds.shape}\")\n",
    "print(f\"inds_test shape: {test_inds.shape}\")\n",
    "print(f\"X[inds_train, :] shape: {wLFPs[train_inds, :].shape}\")\n",
    "print(f\"Y[inds_train] shape: {flightID[train_inds, 1].shape}\")\n",
    "print(f\"X[inds_test, :] shape: {wLFPs[test_inds, :].shape}\")\n",
    "print(f\"Y[inds_test] shape: {flightID[test_inds, 1].shape}\")\n",
    "\n",
    "\n",
    "titles = ['Projection (real part)', 'Amplitude', 'Softmax 1', 'Softmax 2 (Output)'];\n",
    "for i in range(axs.shape[0]):\n",
    "    print(f\"Training network {i + 1} of {axs.shape[0]} (hidden layer size {3 * 2 ** i})\")  # try 4 different hidden layer sizes\n",
    "    m, _, _ = TIMBRE(wLFPs, flightID[:, 1], test_inds, train_inds, hidden_nodes=3, learn_rate=0.001, is_categorical=True, verbosity=1)\n",
    "    for j in range(axs.shape[1]):  # Loop through each layer\n",
    "        p = helpers.layer_output(wLFPs[test_inds], m, j)  # Calculate layer's response to input, using only test data\n",
    "        if j == 0:\n",
    "            p = p[:, :p.shape[1] // 2]  # just get real component for complex-valued output\n",
    "            axs[i, 0].set_ylabel(str(3 * 2 ** i) + ' features');\n",
    "        if i == 0:\n",
    "            axs[0, j].set_title((titles[j]));\n",
    "        #axs[i, j].plot(helpers.accumarray(arm_and_pos_binned[test_inds], p));  # plot mean response of layer to test data as a function of position\n",
    "        axs[i, j].autoscale(enable=True, axis='both', tight=True);\n",
    "        if i < axs.shape[0] - 1:\n",
    "            axs[i, j].set_xticks([]);\n",
    "        else:\n",
    "            axs[i, j].set_xlabel('Position');\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
