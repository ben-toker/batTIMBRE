{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdf5storage\n",
    "from helpers import *\n",
    "from get_data import *\n",
    "from synchrony import *\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in the bat data (LFP and positional data)\n",
    "#### We first need to load in the LFP data, which in this case is stored in a MATLAB file. We can do this using ```hdf5storage```. The bat's positional data is stored in a matlab file (not accessible for public use), but luckily the accessors for this data can be found in ```dataset.py``` thanks to the Yartsev Lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './bat/data'\n",
    "bat_id = '32622'\n",
    "date = '231007'\n",
    "lfp_file_path = './bat/data/ephys/32622_231007_lfp.mat'\n",
    "\n",
    "\n",
    "#Clean up position data (remove NaNs, etc.) and load in LFP from given file path\n",
    "lfp_mat, cleaned_pos, session = load_and_clean_bat_data(data_path, bat_id, date, lfp_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time synchronization\n",
    "#### Before we get to the main attraction (the LFP data), we need to ensure our data is synchronized. To do this, we need to extract global timestamps from both the LFP and positional data and make sure they start at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfp_timestamps_edges, binned_pos, pos_timestamps, lfp_indices, valid_indices = sync_and_bin_data(lfp_mat, session)\n",
    "\n",
    "#lfp_timestamp_edges stores edges between timebins. this will be useful for aligning the LFP data with the position data\n",
    "#binned_pos is the cleaned position averaged over the timebins\n",
    "#valid_indices is a boolean array that marks the non-negative position timestamps\n",
    "#pos_timestamps is the cleaned and filtered timestamps of the position data\n",
    "#lfp_indices is a boolean array that marks the non-negative, decimated LFP timestamps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inside of ```lfp_timestamps_edges```, we store the *edges* between timebins. We will use this to later to bin the position data; instead of downsampling the data like we did the LFP, we will average across bins (between two edges) of the LFP timebins to get synchronized data streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First few elements of binned_pos:\\n\", binned_pos[:, :5]) # NaN values at beginning and end are expected; position is not recorded when bat is not visible.\n",
    "\n",
    "print(\"First few LFP bins:\", lfp_timestamps_edges[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notice above that the LFP timestamp edges have N+1 the shape of the binned position. This makes sense and is expected; `lfp_timestamps_edges` contains the bins (which are stored in groups of two, i.e. the first bin is [0, 4514.4426] and so on) for which the position was binned into."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organizing behavioral data\n",
    "#### To better organize the binned flight data, we need to construct a flightID array which will contain all the binned positions for each flight, accounting for which feeder (or the perch) was visited for each data point entered in that flight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_data import get_flightID\n",
    "\n",
    "flightID = get_flightID(session, binned_pos, valid_indices, lfp_timestamps_edges, pos_timestamps, off_samples = 125) #includes the 5 seconds before and after flight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LFP extraction and downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfp_bat_combined = extract_and_downsample_lfp_data(lfp_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imported raw LFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synchrony import plot_raw_lfp\n",
    "\n",
    "# Example usage\n",
    "plot_raw_lfp(lfp_bat_combined, n_channels=192, start_time=0, end_time=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once LFP is loaded in and downsampled, we can apply a filter and Hilbert transform to get our complex-valued LFP!\n",
    "\n",
    "#### *Note: At 25hz, a signal has at most 12.5hz frequency of usable data. Given this property, we don't need to do a bandpass filter (to cap out the high and low range). As such, we only need to do a highpass filter at 1hz.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFPs = filter_data(lfp_bat_combined, 1, fs=25, filt_type='high', use_hilbert=True) \n",
    "LFPs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFPs = LFPs[lfp_indices]\n",
    "LFPs.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We now have our processed LFP. `LFPs` contains the filtered and (Hilbert) transformed LFP data for all of the valid `binned_pos` entries. However, we are mainly interested in the bat flights, which are just a *fraction* of the total of `binned_pos`. To filter out the non-flight entries from the LFP, we will apply a similar filtering method as we did in `get_flightID` with a `get_flightLFP` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_data import get_flightLFP\n",
    "\n",
    "flightLFP = get_flightLFP(session, LFPs, valid_indices, lfp_timestamps_edges, pos_timestamps, off_samples=125) # Make sure off_samples is the same for flightID and flightLFP.\n",
    "flightLFP.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying TIMBRE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from TIMBRE import TIMBRE\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Define class names matching your labels (labels from 0 to 5)\n",
    "class_names = [\n",
    "    'Perch to Feeder 1',       # Label 0\n",
    "    'Feeder 1 to Perch',       # Label 1\n",
    "    'Perch to Feeder 2',       # Label 2\n",
    "    'Feeder 2 to Perch',       # Label 3\n",
    "    'Feeder 1 to Feeder 2',    # Label 4\n",
    "    'Feeder 2 to Feeder 1'     # Label 5\n",
    "]\n",
    "\n",
    "# Initialize parameters\n",
    "n_folds = 5\n",
    "hidden_sizes = [3, 6, 12, 24]\n",
    "all_accuracies = []\n",
    "all_cm = {}\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    print(f\"\\nEvaluating hidden size: {hidden_size} nodes\")\n",
    "    fold_accuracies = []\n",
    "    cm_total = None\n",
    "\n",
    "    for which_fold in range(n_folds):\n",
    "        print(f\"  Fold {which_fold + 1}/{n_folds}\")\n",
    "\n",
    "        # Get train and test indices\n",
    "        test_inds, train_inds = test_train_bat(flightID, n_folds, which_fold)\n",
    "\n",
    "        # Whiten LFPs\n",
    "        wLFPs, _, _ = whiten(LFPs, train_inds)\n",
    "\n",
    "        # Adjust labels to start from 0\n",
    "        labels = flightID[:, 1].astype(int) - 1\n",
    "\n",
    "        # Train the TIMBRE model\n",
    "        m, _, _ = TIMBRE(\n",
    "            wLFPs, labels, test_inds, train_inds,\n",
    "            hidden_nodes=hidden_size, is_categorical=True\n",
    "        )\n",
    "\n",
    "        # Get predictions on test data\n",
    "        output_layer = layer_output(wLFPs[test_inds], m, -1)\n",
    "        predictions = np.argmax(output_layer, axis=1)\n",
    "        true_labels = labels[test_inds]\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = np.mean(predictions == true_labels)\n",
    "        fold_accuracies.append(accuracy)\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        labels_list = np.arange(len(class_names))  # Labels from 0 to 5\n",
    "        cm = confusion_matrix(\n",
    "            true_labels, predictions, labels=labels_list\n",
    "        )\n",
    "\n",
    "        # Accumulate confusion matrices\n",
    "        if cm_total is None:\n",
    "            cm_total = cm\n",
    "        else:\n",
    "            cm_total += cm\n",
    "\n",
    "        # Classification report\n",
    "        report = classification_report(\n",
    "            true_labels, predictions,\n",
    "            labels=labels_list,\n",
    "            target_names=class_names, zero_division=0\n",
    "        )\n",
    "        print(f\"Classification Report for Fold {which_fold + 1}:\\n{report}\")\n",
    "\n",
    "    # Average accuracy\n",
    "    avg_accuracy = np.mean(fold_accuracies)\n",
    "    all_accuracies.append(avg_accuracy)\n",
    "    all_cm[hidden_size] = cm_total / n_folds\n",
    "\n",
    "    print(f\"Average accuracy for hidden size {hidden_size}: {avg_accuracy:.4f}\")\n",
    "\n",
    "# Plot average accuracy vs hidden layer size\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(hidden_sizes, all_accuracies, marker='o')\n",
    "plt.title('Model Accuracy vs Hidden Layer Size')\n",
    "plt.xlabel('Number of Hidden Nodes')\n",
    "plt.ylabel('Average Accuracy over Folds')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot average confusion matrices for each hidden size\n",
    "fig, axs = plt.subplots(2, 2, figsize=(20, 15))\n",
    "fig.suptitle('Average Confusion Matrices for Bat Flight End Position Prediction', fontsize=16)\n",
    "\n",
    "for idx, hidden_size in enumerate(hidden_sizes):\n",
    "    cm_avg = all_cm[hidden_size]\n",
    "\n",
    "    # Normalize confusion matrix to show percentages\n",
    "    cm_normalized = cm_avg.astype('float') / cm_avg.sum(axis=1)[:, np.newaxis]\n",
    "    cm_normalized = np.nan_to_num(cm_normalized)  # Replace NaNs with zeros if any class has zero samples\n",
    "\n",
    "    ax = axs[idx // 2, idx % 2]\n",
    "    sns.heatmap(\n",
    "        cm_normalized, annot=True, fmt='.2f', ax=ax,\n",
    "        xticklabels=class_names, yticklabels=class_names, cmap='Blues'\n",
    "    )\n",
    "    ax.set_title(f'Confusion Matrix (Hidden Nodes: {hidden_size})')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_pos_bat(positions, n_bins, train_inds):\n",
    "    min_pos = np.min(positions[train_inds], axis=0)\n",
    "    max_pos = np.max(positions[train_inds], axis=0)\n",
    "    return np.floor((positions - min_pos) / (max_pos - min_pos) * n_bins).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from TIMBRE import TIMBRE\n",
    "import numpy as np\n",
    "import helpers  # Assuming you have a helpers module\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "fig, axs = plt.subplots(4, 4, figsize=(20, 15))\n",
    "fig.suptitle('TIMBRE Model Performance for Bat Flight Position Prediction', fontsize=16)\n",
    "\n",
    "n_folds = 5\n",
    "which_fold = 0\n",
    "n_bins = 20  # Adjust as needed\n",
    "\n",
    "# Step 1: Obtain test and train indices\n",
    "test_inds, train_inds = test_train_bat(flightID, n_folds, which_fold)\n",
    "\n",
    "# Step 2: Whiten the LFPs\n",
    "wLFPs, _, _ = helpers.whiten(LFPs, train_inds)\n",
    "\n",
    "# Step 3: Extract positions and apply PCA\n",
    "positions = flightID[:, 2:5]  # X, Y, Z positions\n",
    "pca = PCA(n_components=1)\n",
    "positions_1d = pca.fit_transform(positions).flatten()\n",
    "\n",
    "# Step 4: Bin the 1D positions\n",
    "pos_bins = np.linspace(positions_1d.min(), positions_1d.max(), n_bins + 1)\n",
    "pos_binned = np.digitize(positions_1d, bins=pos_bins) - 1\n",
    "labels = pos_binned\n",
    "\n",
    "# Step 5: Training and Plotting\n",
    "titles = ['Projection (real part)', 'Amplitude', 'Softmax 1', 'Softmax 2 (Output)']\n",
    "for i in range(axs.shape[0]):\n",
    "    hidden_nodes = 3 * 2 ** i\n",
    "    print(f\"Training network {i + 1} of {axs.shape[0]} (hidden layer size {hidden_nodes})\")\n",
    "    \n",
    "    # Train the TIMBRE model\n",
    "    m, _, _ = TIMBRE(wLFPs, labels, test_inds, train_inds, hidden_nodes=hidden_nodes)\n",
    "    \n",
    "    for j in range(axs.shape[1]):\n",
    "        # Calculate layer's response to input, using only test data\n",
    "        p = helpers.layer_output(wLFPs[test_inds], m, j)\n",
    "        \n",
    "        if j == 0:\n",
    "            p = p[:, :p.shape[1] // 2]\n",
    "            axs[i, 0].set_ylabel(f'{hidden_nodes} features')\n",
    "        \n",
    "        if i == 0:\n",
    "            axs[0, j].set_title(titles[j])\n",
    "        \n",
    "        # Compute mean response per position bin\n",
    "        mean_response = helpers.accumarray(labels[test_inds], p)\n",
    "        \n",
    "        # Plot the mean response\n",
    "        axs[i, j].plot(mean_response)\n",
    "        axs[i, j].autoscale(enable=True, axis='both', tight=True)\n",
    "        \n",
    "        if i < axs.shape[0] - 1:\n",
    "            axs[i, j].set_xticks([])\n",
    "        else:\n",
    "            axs[i, j].set_xlabel('Position along Principal Component')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
